---
title: "Journal (reproducible report)"
author: "Alexander Sowarka"
date: "2020-11-30"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

# Intro to tidyverse Challenge

Last compiled: `r Sys.Date()`

## Given Code
Following given code chunks from the exercise are used:
```{r}
# Data Science at TUHH ------------------------------------------------------
# SALES ANALYSIS ----

# 1.0 Load libraries ----
library(tidyverse)
library(readxl)
library(lubridate)

# 2.0 Importing Files ----
# A good convention is to use the file name and suffix it with tbl for the data structure tibble
bikes_tbl      <- read_excel(path = "00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel("00_data/01_bike_sales/01_raw_data/orderlines.xlsx")

# Not necessary for this analysis, but for the sake of completeness
bikeshops_tbl  <- read_excel("00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")

# 3.0 Examining Data ----


# 4.0 Joining Data ----
bike_orderlines_joined_tbl <- orderlines_tbl %>%
  left_join(bikes_tbl, by = c("product.id" = "bike.id")) %>%
  left_join(bikeshops_tbl, by = c("customer.id" = "bikeshop.id"))

# Examine the results with glimpse()
bike_orderlines_joined_tbl %>% glimpse()

# 5.0 Wrangling Data ----
# All actions are chained with the pipe already. You can perform each step separately and use glimpse() or View() to validate your code. Store the result in a variable at the end of the steps.
bike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%
  # 5.1 Separate category name
separate(col    = category,
         into   = c("category.1", "category.2", "category.3"),
         sep    = " - ") %>%

# 5.2 Add the total price (price * quantity) 
# Add a column to a tibble that uses a formula-style calculation of other columns
mutate(total.price = price * quantity) %>%

# 5.3 Optional: Reorganize. Using select to grab or remove unnecessary columns
# 5.3.1 by exact column name
select(-...1, -gender) %>%

# 5.3.2 by a pattern
# You can use the select_helpers to define patterns. 
# Type ?ends_with and click on Select helpers in the documentation
select(-ends_with(".id")) %>%

# 5.3.3 Actually we need the column "order.id". Let's bind it back to the data
bind_cols(bike_orderlines_joined_tbl %>% select(order.id)) %>% 

# 5.3.4 You can reorder the data by selecting the columns in your desired order.
# You can use select_helpers like contains() or everything()
select(order.id, contains("order"), contains("model"), contains("category"),
       price, quantity, total.price,
       everything()) %>%

# 5.4 Rename columns because we actually wanted underscores instead of the dots
# (one at the time vs. multiple at once)
rename(bikeshop = name) %>%
set_names(names(.) %>% str_replace_all("\\.", "_"))
```

## Data Wrangling

For the challenge first the data wrangling needs to be adapted to separate the location attribute into city and state:

```{r}
bike_orderlines_wrangled_tbl <- bike_orderlines_wrangled_tbl %>%
# 5.5 Separate Location into State and City
separate(col    = location,
         into   = c("city", "state"),
         sep    = ", " )
bike_orderlines_wrangled_tbl
```

## Buiseness Insights - Sales by State
### Data Manipulation
The data must be grouped by the new created state column and then the sales must be summarised. This results in the sales per state.
```{r}
sales_by_loc_tbl <- bike_orderlines_wrangled_tbl %>%
  
# Select state and price
select(state, total_price) %>%

# Group by State
group_by(state) %>%
summarise(sales = sum(total_price)) %>%
ungroup() %>%

# Format $ Text
mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                   decimal.mark = ",", 
                                   prefix = "", 
                                   suffix = " €"))

sales_by_loc_tbl
```

### Data Visualization
For plotting the following code can be used:
```{r, fig.width=10, fig.height=7}
sales_by_loc_tbl %>%
  
# Setup canvas with the columns year (x-axis) and sales (y-axis)
ggplot(aes(x = state, y = sales)) +

# Geometries
geom_col(fill = "#2DC6D6") + # Use geom_col for a bar plot

  # Theme
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +

# Formatting
scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                  decimal.mark = ",", 
                                                  prefix = "", 
                                                  suffix = " €")) +
labs(
  title = "Revenue by State",
  x = "State", # Override defaults for x and y
  y = "Revenue"
)
```

## Buiseness Insights - Sales by State and Year
### Data Manipulation
The data must be grouped by the new created state column and newly created year column and then the sales must be summarised. This results in the sales per state and year.
```{r}
sales_by_loc_year_tbl <- bike_orderlines_wrangled_tbl %>%
  
# Select state and price and add a year
select(order_date, state, total_price) %>%
mutate(year = year(order_date)) %>%

# Group by State
group_by(year, state) %>%
summarise(sales = sum(total_price)) %>%
ungroup() %>%

# Format $ Text
mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                   decimal.mark = ",", 
                                   prefix = "", 
                                   suffix = " €"))

sales_by_loc_year_tbl  
```

### Data Visualization
For plotting the following code can be used:
```{r, fig.width=10, fig.height=7}
sales_by_loc_year_tbl %>%
  
  # Setup canvas with the columns year (x-axis) and sales (y-axis)
  ggplot(aes(x = year, y = sales, fill = state)) +
  
  # Geometries
  geom_col() + # Use geom_col for a bar plot
  
  # Facet
  facet_wrap(~ state) +
  
  # Theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  
  # Formatting
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title = "Revenue by State",
    x = "", # Override defaults for x and y
    y = "Revenue",
    fill = "State"
  )
```

# Data Acquisition Challenge


## API Challeng
## Used Libs
```{r}
#include packages
library(httr)
library(glue)
library(tidyverse)
library(jsonlite)
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(stringi)   # character string/text processing
# 1.1 COLLECT PRODUCT FAMILIES ----
```

## Chuck Norris API
In the following jokes from the Chuck Norris Web API are requested (https://api.chucknorris.io/). First the call to the API is wrapped in a function and the response is checked for its format. It is a normal JSON object.
```{r}
# Wrapped Chuck Norris API into a function
chuck_api <- function(path) {
  url <- modify_url(url = "https://api.chucknorris.io", path = glue("/jokes{path}"))
  resp <- GET(url)
  stop_for_status(resp) # automatically throws an error if a request did not succeed
  return (resp)
}
  #check response code
  resp <- chuck_api("/random")
  resp
  
  #check content
  resp %>% 
    .$content %>% 
    rawToChar()
    

```

Using this API one can define a function to directly get a random joke from a requested JSON Object.
```{r}
# function to get random joke from API
get_joke <- function() {
  joke_json <- chuck_api("/random") %>% 
  .$content %>% 
  rawToChar() %>% 
  fromJSON()
  return (joke_json$value)
}

get_joke()
```

The site offers also different categories for jokes.
```{r}
content(chuck_api("/categories"), as = "parsed")
```

With the following query you can directly get a joke from the "dev" category.
```{r}
category="dev"
content(chuck_api(glue("/random?category={category}")), as = "parsed")%>% 
  .$value
```

## Web Scraping of Radon
In the following data about the bicycles from Radon is retrieved from their website. First we retrieve all main categories from their site (https://www.radon-bikes.de/en/). Instead of using the banner at the top, we use the buttons further down to identify the categories:
```{r}
url_home          <- "https://www.radon-bikes.de/en/"

# Read in the HTML for the entire webpage
html_home         <- read_html(url_home)

# Web scrape the ids for the families
radon_family_tbl <- html_home %>%
  
  # Get the nodes for the families ...
  html_nodes(css = ".a-panel--light") %>%
  html_nodes(css = ".a-button--margin-top-small") %>%
  html_attr('href') %>%
  
  # Convert vector to tibble
  enframe(name = "position", value = "subdirectory") %>%
  
  #filter other urls
  filter(!(subdirectory %>% str_detect("www."))) %>%

  # Add the domain, because we will get only the subdirectories
   mutate(
    url = glue("https://www.radon-bikes.de{subdirectory}")
  ) %>%

  # Some categories are listed multiple times.
  # We only need unique values
  distinct(url)


radon_family_tbl
```

Some categories like E-bike are further divided into subcategories. They are directly retrieved from the buttons on the site of the main category. The following function gives all relevant links to the subcategories of a main category. This function uses that main categories without subcategories don't have a button to switch to a subcategory. For a main category without a sub category only the main category is returned again. The first provided example retrieves the subcategory links for the main category "Mountainbike" 
```{r}
#function for bike dubcategories
get_bike_data_sub_category <- function(url) {
  sub_node <- read_html(url) %>%
    
  #Check for link to subcategory
  html_nodes(css = ".m-teaser-grid__linkcontainer") %>%
  html_attr('href') %>%
  
  # Convert vector to tibble
  enframe(name = "position", value = "subdirectory") %>%
  mutate(
      url = glue("https://www.radon-bikes.de{subdirectory}")
    ) %>%
    
    # Some categories are listed multiple times.
    # We only need unique values
  distinct(url)
  
  if (dim(sub_node)[1] != 0)
    return (sub_node)
  else
    return (tibble(url))
}



sub_node <- get_bike_data_sub_category(radon_family_tbl$url[1])
sub_node
```

The second example retrieves the bucategory for "Trekking". Trelling does not have a subcategory, so the link to the main category is returned again.
```{r}
sub_node <- get_bike_data_sub_category(radon_family_tbl$url[2])
sub_node
```

Attaching "bikegrid" to the url of a category leads to having an overview of all available bicycles for this category. The following function then returns a link to all individual bicycles of a (sub-)categorty. Using the ".gearhub-1" CSS also automatically filters out any non bicycle (as they don't have gears)


```{r}
get_bike_data_url<- function(url) {
  # All bikes are under bikegrid
  all_bike_html <- glue("{url}bikegrid/")
  bike_url <- read_html(all_bike_html) %>%
    
    #Only bikes gave gears
    html_nodes(css = ".gearhub-1") %>%
    html_nodes(css = "a") %>%
    html_attr('href') %>%
    # 
    # Convert vector to tibble
    enframe(name = "position", value = "subdirectory") %>%
    mutate(
      url = glue("https://www.radon-bikes.de{subdirectory}")
    ) %>%
    
    # Some categories are listed multiple times.
    # We only need unique values
    distinct(url)
  return (bike_url)
}

bike_data_url <- get_bike_data_url("https://www.radon-bikes.de/en/mountainbike/hardtail/")
bike_data_url
```

The following function can then retrieve data for an individual bicycle. The following data is retrieved: Name (retrieved from a heading inside the html), the description (from a html paragraph), availability and price. Availability and price is retrieved using a regex in part of a script of the html. The category should be privided from the calling function to complete the tibble entry.
```{r}
get_bike_data <-function(url, category) {
  bike_html <- read_html(url)
  bike_name <- 
    bike_html %>%
    
    #Only bikes gave gears
    html_node(css = ".a-heading--medium") %>%
    html_text()
  
  bike_descr_text <- bike_html %>%
    
    #Only bikes gave gears
    html_node(css = ".a-paragraph--bigger") %>%
    html_text()
  
  bike_script_text <- bike_html %>%
    
    #Only bikes gave gears
    html_nodes(css = ".mod-bikedetail") %>%
    
    #get first script node
    html_node(css = "script") %>%
    html_text()
  
  
  available <- stringr::str_extract(bike_script_text, "availability.*?\\{.*?\\}") %>%
    str_detect("true")
  price <- stringr::str_match(bike_script_text, "eur.*?price.*?(\\d+)")
  price <- price[2]
  
    
  return (tibble(name =  bike_name,category = category, available = available, price = as.numeric(price), descr = bike_descr_text, url = url))
}

bike_data <- get_bike_data("https://www.radon-bikes.de/en/mountainbike/hardtail/jealous/jealous-80-2021/", "a")
bike_data
```

In the last part we put all functions together in a wrapper function to retrieve all data for a category. The category for the individual bike is retrieved from the url. First the urls of the subcategories is retrieved. Then the urls of all bicycles, then the additional data from the bicycle pages. The subcategory is discarded as information and only the main category is kept. This new function can then be used to retrieve data from the complete website. This code is not executed because of the long runtime.
```{r eval=FALSE}
get_bike_data_per_cat <- function(url) {
  category <- url %>%
    str_replace(url_home,"") %>%
    str_replace("/","")
  url_per_subcat_vec <- get_bike_data_sub_category(url) %>% pull(url)
  url_per_bike <- url_per_subcat_vec %>% map(get_bike_data_url) %>% bind_rows %>% pull(url)

  bike_data_cat_list <- url_per_bike %>% map(get_bike_data, category = category)
  return (bind_rows(bike_data_cat_list))
}


#plan("multiprocess")
radon_bike_lst <- radon_family_tbl %>% pull(url) %>% map(get_bike_data_per_cat)
radon_bike_tbl <- radon_bike_lst %>% bind_rows()
```

The final data is displayed using saved data of a previous run:   
```{r eval=TRUE}
web_scraping_radon_data <- readRDS("H:/Uni/Buiseness Data Science/Repo/WS20_Repo/web_scraping_radon_data.rds")
web_scraping_radon_data
```


# Data Wrangling Challenge

## Further needed libraries
The following libraries are needed:
```{r}
library(vroom)
library(data.table)
```

## Loading of data from the tables
This code shows how to load the .tsv files and load the data into data.table objects. These objects are then saved in .rds files. This code is not executed due to the high execution time. Unnecessary columns for this challenge are skipped.

```{r eval=FALSE}
library(vroom)
library(data.table)
col_types <- list(
  id = col_character(),
  type = col_skip(),
  number = col_character(),
  country = col_character(),
  date = col_date("%Y-%m-%d"),
  abstract = col_skip(),
  title = col_skip(),
  kind = col_skip(),
  num_claims = col_skip(),
  filename = col_skip(),
  withdrawn = col_skip()
)

patent_tbl <- vroom(
  file       = "./02_data_wrangling/patent.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

setDT(patent_tbl)
saveRDS(patent_tbl,"./02_data_wrangling/patent.rds")

col_types <- list(
  id = col_character(),
  type = col_integer(),
  name_first = col_skip(),
  name_last = col_skip(),
  organization = col_character()
)

assignee_tbl <- vroom(
  file       = "./02_data_wrangling/assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

setDT(assignee_tbl)
saveRDS(assignee_tbl,"./02_data_wrangling/assignee.rds")

col_types <- list(
  id = col_character()
)

mainclass_current_tbl <- vroom(
  file       = "./02_data_wrangling/mainclass_current.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

setDT(mainclass_current_tbl)
saveRDS(mainclass_current_tbl,"./02_data_wrangling/mainclass_current.rds")

col_types <- list(
  patent_id = col_character(),
  assignee_id = col_character(),
  location_id = col_character()
)

patent_assignee_tbl <- vroom(
  file       = "./02_data_wrangling/patent_assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

setDT(patent_assignee_tbl)
saveRDS(patent_assignee_tbl,"./02_data_wrangling/patent_assignee.rds")


col_types <- list(
  uuid = col_character(),
  patent_id = col_character(),
  mainclass_id = col_character(),
  subclass_id = col_character(),
  sequence = col_skip()
)

uspc_tbl <- vroom(
  file       = "./02_data_wrangling/uspc.tsv", 
  delim      = "\t", 
  col_types  = col_types,
  na         = c("", "NA", "NULL")
)

setDT(uspc_tbl)
saveRDS(uspc_tbl,"./02_data_wrangling/uspc.rds")

```

### Loading the data from .rds files
To use the data it is now loaded from the .rds files and keys are set.
```{r eval=TRUE}
patent_tbl <- readRDS("./02_data_wrangling/patent.rds")
assignee_tbl <- readRDS("./02_data_wrangling/assignee.rds")
mainclass_current_tbl <- readRDS("./02_data_wrangling/mainclass_current.rds")
patent_assignee_tbl <- readRDS("./02_data_wrangling/patent_assignee.rds")
uspc_tbl <- readRDS("./02_data_wrangling/uspc.rds")

setkey(patent_tbl, id)
setkey(assignee_tbl, id)
setkey(mainclass_current_tbl, id)
setkey(patent_assignee_tbl, patent_id)
setkey(uspc_tbl, uuid)
```

## Top US Companies

To get the top US companies with the most patents, we first create a data table for all patents that are assigned to a company. For this specific challenge we need only US companies. For the further challenges we also need to consider companies worldwide. To achieve this we join the table "assignee" and "patent_assignee". To filter for US companies we can filter for type 2. Worldwide companies are either US (type = 2) or foreign (type = 3).

```{r}
company_assigned_patent_tbl <- assignee_tbl[patent_assignee_tbl, on = c(id = "assignee_id")][type == 2]
company_assigned_patent_world <- assignee_tbl[patent_assignee_tbl, on = c(id = "assignee_id")][type == 2 | type == 3]
```

To get the top 10 companies we can now do the following:

  1.    Group By organization
  2.    Count the number of patents
  3.    Order  by number of patents
  4.    Get first 10 entries
  
```{r}
top_company_tbl <- company_assigned_patent_tbl[, .(numOfPatents = .N), by = organization][order(-numOfPatents)][1:10]

top_company_tbl
```

## Top US Companies 2019

To only consider data from 2019, we need to evaluate the table "patent". Here a date for the assignment is given. We can join our previously created table with the US companies with the "patent" table and then filter by the year 2019.

```{r}
top_company_tbl_2019 <- patent_tbl[company_assigned_patent_tbl, on = c(number = "patent_id")][lubridate::year(date) == 2019][
                                      , .(numOfPatents = .N), by = organization][order(-numOfPatents)][1:10]
top_company_tbl_2019
```

## Top 5 USPTO classes

Approach:

  1.    Get top 10 companies worldwide
  2.    Filter the table with companies and patents, to only get patents from these top 10
  3.    Join with table "uspc"
  4.    Group By mainclass_id
  5.    Count patents per mainclass
  6.    Select top 5
  7.    Join table "mainclass" to get title of mainclass

Top 10 companies worldwide:

```{r}
top_company_world <- company_assigned_patent_world[, .(numOfPatents = .N), by = organization][order(-numOfPatents)][1:10]
top_company_world
```

Top 5 categories:

```{r}
top_company_patents <- company_assigned_patent_world[organization %in% top_company_world[,organization]]

top_uspc_id_tbl <- top_company_patents[uspc_tbl, on = c(patent_id = "patent_id"), nomatch=0][, 
                                      .(patentsPerCat = .N), by = mainclass_id][order(-patentsPerCat)][1:5]
top_uspc_tbl <- mainclass_current_tbl[top_uspc_id_tbl, on = c(id="mainclass_id")]

top_uspc_tbl
```

# Data Visualization

## Data Preperation

First the COVID data is read and transformed to match the region format in the world table. To get the cumulative cases the data is grouped by country, arranged by date, and then the cumulative sum over the cases is computed. We filter for the countries we are interested in:

```{r}
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv") %>% 
  mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
  mutate(countriesAndTerritories = case_when(
    
    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories))

covid_cases_per_country_month_tbl <- covid_data_tbl %>% 
  group_by(countriesAndTerritories) %>% 
  arrange(year,month,day) %>%
  summarise(cases_per_month = cumsum(cases), year = year, month = month, day = day) %>% 
  ungroup() %>% 
  filter(countriesAndTerritories %in% c("Germany", "UK", "France", "Spain", "USA" )) %>% 
  rename(region = countriesAndTerritories)

```

To get the cases for Europe, we first compute the total cases per day per continent by grouping by the continent and then computing the sum. Then we can proceed as if Europe was a single country. We rename the column in the result to have matching columns in both computed tables. This enables us to bind them together. Finally we filter for data out of 2020.

```{r}
covid_cases_eur_per_month_tbl <- covid_data_tbl %>% 
  group_by(continentExp, year, month, day) %>% 
  summarise(cases_per_continent = sum(cases)) %>%
  ungroup()  %>%
  group_by(continentExp) %>%
  arrange(year,month,day) %>%
  summarise(cases_per_month = cumsum(cases_per_continent), year = year, month = month, day = day) %>%
  ungroup() %>%
  filter(continentExp == "Europe") %>%
  rename(region = continentExp)

covid_cases_per_region_month_tbl <- bind_rows(covid_cases_eur_per_month_tbl, covid_cases_per_country_month_tbl) %>%
  filter(year==2020)
```

## Line Plot

Now we can plot the data with the following code:

```{r, fig.width=10, fig.height=7}
library(scales)

covid_cases_per_region_month_tbl %>%
  ggplot(aes(x = as.Date(paste(year, month, day, sep='-')), y = cases_per_month, color = region, fill = region)) +
  geom_line(size = 1.1, linetype = 1) +
  geom_label(aes(label =  scales::dollar(cases_per_month, 
                                           prefix = "",
                                           suffix = "")),
             nudge_x= -3, 
             size  = 3,
             color = "white",
             fontface = "italic",
             data = covid_cases_per_region_month_tbl %>% 
               filter(region %in% c("USA", "Europe") & year == 2020 & month == 12 & day == 4),
             show.legend=F) +
  scale_color_brewer(palette="Spectral") +
  scale_fill_brewer(palette="Spectral") +
  expand_limits(y = 20e6, x = as.Date("2020-12-16")) +
  scale_x_date(date_breaks = "1 month", minor_breaks = NULL, date_labels = "%B", expand = c(0,0))  +
  scale_y_continuous(labels = unit_format(unit = "M" , scale =1e-6), expand = c(0,0)) +
  
  
  labs(
    title = "COVID-19 confimed cases worldwide",
    subtitle = "As of 12/05/2020 Europe has more cases than USA",
    x = "Year 2020", # Override defaults for x and y
    y = "Cumulative Cases",
    color = "Continent/Country"
  )+
  
# Theme
theme_light() +
  theme(
    title = element_text(face = "bold", color = "#08306B"),
    axis.text.x = element_text(angle = 45, hjust = 1), legend.position="bottom", 
    plot.subtitle=element_text(size=8, face="italic", color="black"))
```

## World Plot

To create the heatmap, we first get the world latitude/longitude data. We compute the mortality rate, by summing deaths and then computing the ratio. We join this computed table with the world table and plot the map. The mortality rate is our scale for the gradient color.

```{r}
library(maps)
world <- map_data("world")

covid_mortalitly_tbl <- covid_data_tbl %>%
  group_by(countriesAndTerritories, popData2019) %>%
  summarise(deaths_per_region = sum(deaths)) %>%
  ungroup() %>%
  mutate(mortality = (deaths_per_region/popData2019))

covid_world <- left_join(world,covid_mortalitly_tbl, by = c("region" = "countriesAndTerritories"))

covid_world %>% ggplot(aes(x = long, y = lat)) +
  geom_map(aes(map_id=region, fill=mortality), map = world) +
  scale_fill_gradient2(
  low   = "indianred1",
  mid  = "darkred",
  high = "black",
  midpoint = 0.0011,
  labels = percent,
  limits = c(0,0.0015)) +
  labs(
    title = "Confirmed COVID-19 deaths relative to the size of the population",
    subtitle = "More than 1.2 Million confirmed deaths worldwide",
    x = "", # Override defaults for x and y
    y = ""
  ) + 
  theme(
    title = element_text(face = "bold", color = "#08306B"),
    plot.subtitle=element_text(size=8, face="italic", color="black"))
```